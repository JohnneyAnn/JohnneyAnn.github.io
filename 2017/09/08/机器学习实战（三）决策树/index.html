<!DOCTYPE html>
<html lang="">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="机器学习实战（三）决策树"/>




  <meta name="keywords" content="机器学习,Python," />





  <link rel="alternate" href="/atom.xml" title="Russell">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1" />



<link rel="canonical" href="http://JohnneyAnn.github.io/2017/09/08/机器学习实战（三）决策树/"/>


<meta name="description" content="决策树是一种简单高效并且具有强解释性的模型，广泛应用于数据分析领域。其本质是一颗由多个判断节点组成的树，在使用模型进行预测时，根据输入参数依次在各个判断节点进行判断游走，最后到叶子节点即为预测结果。">
<meta name="keywords" content="机器学习,Python">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习实战（三）决策树">
<meta property="og:url" content="http://JohnneyAnn.github.io/2017/09/08/机器学习实战（三）决策树/index.html">
<meta property="og:site_name" content="Russell">
<meta property="og:description" content="决策树是一种简单高效并且具有强解释性的模型，广泛应用于数据分析领域。其本质是一颗由多个判断节点组成的树，在使用模型进行预测时，根据输入参数依次在各个判断节点进行判断游走，最后到叶子节点即为预测结果。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://johnneyann.github.io/img/tree/output_9_0.png">
<meta property="og:image" content="http://johnneyann.github.io/img/tree/output_10_1.png">
<meta property="og:image" content="http://johnneyann.github.io/img/tree/output_11_0.png">
<meta property="og:image" content="http://johnneyann.github.io/img/tree/output_13_0.png">
<meta property="og:updated_time" content="2017-09-08T14:05:57.088Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习实战（三）决策树">
<meta name="twitter:description" content="决策树是一种简单高效并且具有强解释性的模型，广泛应用于数据分析领域。其本质是一颗由多个判断节点组成的树，在使用模型进行预测时，根据输入参数依次在各个判断节点进行判断游走，最后到叶子节点即为预测结果。">
<meta name="twitter:image" content="http://johnneyann.github.io/img/tree/output_9_0.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1" />
<link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet'>


  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" />




<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: true
    },
  };
</script>




  



    <title> 机器学习实战（三）决策树 - Russell </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">Russell</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          机器学习实战（三）决策树
        
      </h1>

      <time class="post-time">
          9月 08 2017
      </time>
    </header>



    
            <div class="post-content">
            <p>决策树是一种简单高效并且具有强解释性的模型，广泛应用于数据分析领域。其本质是一颗由多个判断节点组成的树，在使用模型进行预测时，根据输入参数依次在各个判断节点进行判断游走，最后到叶子节点即为预测结果。<br><a id="more"></a></p>
<pre><code class="python">&quot;&quot;&quot;
文件trees
&quot;&quot;&quot;
from math import log
import operator
%matplotlib inline #这一句设置在线显示

&quot;&quot;&quot;
创建数据集
&quot;&quot;&quot;
def createDataSet():
    dataSet = [[1, 1, &#39;yes&#39;],
              [1, 1, &#39;yes&#39;],
              [1, 0, &#39;no&#39;],
              [0, 1, &#39;no&#39;],
              [0, 1, &#39;no&#39;]]
    labels = [&#39;no surfacing&#39;,&#39;flippers&#39;]
    return dataSet, labels

&quot;&quot;&quot;
计算给定数据集的信息熵(香农)
&quot;&quot;&quot;
def calcuShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    &quot;&quot;&quot;
    为所有可能分能创建字典
    &quot;&quot;&quot;
    for featVec in dataSet:
        currentLabel = featVec[-1]
        #方法1
        #   if currentLabel not in labelCounts.keys():
        #        labelCounts[currentLabel] = 0
        #   labelCounts[currentLabel] += 1
        #方法2
        labelCounts[currentLabel] = labelCounts.get(currentLabel,0) + 1
    shannonEnt = 0.0
    for key in labelCounts:
        &quot;&quot;&quot;
        每个类别所占的比
        &quot;&quot;&quot;
        prob = float(labelCounts[key])/numEntries
        &quot;&quot;&quot;
        求对数
        &quot;&quot;&quot;
        shannonEnt -= prob * log(prob,2)
    return shannonEnt

&quot;&quot;&quot;
按照给定特征划分数据集
三个参数：待划分的数据集、划分数据集的特征、特征的返回值
&quot;&quot;&quot;
def splitDataSet(dataSet, axis, value):
    &quot;&quot;&quot;
    创建新的list对象
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    理解：按axis这列来划分，若这列的数=value归到一类，并创建一个新列表返回
    &quot;&quot;&quot;
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            &quot;&quot;&quot;
            抽取
            &quot;&quot;&quot;
            # 应该是清空列表
            reducedFeatVec = featVec[:axis] 
            # [0+1:]取下标1之后的，这里是取后两位，将元素塞入reducedFeatVec
            reducedFeatVec.extend(featVec[axis+1:]) 
            #将reducedFeatVec列表塞入retDataSet
            retDataSet.append(reducedFeatVec)
    return retDataSet

    &quot;&quot;&quot;
    选择最好的数据集划分方式
    此处使用ID3算法，获取信息增益最大的
    &quot;&quot;&quot;
def chooseBestFeatureToSplit(dataSet):
    #dataSet[0]列数
    #只用前两列进行分类 -1
    numFeatures = len(dataSet[0]) - 1
    baseEntropy = calcuShannonEnt(dataSet)
    #信息增益
    bestInfoGain = 0.0
    bestFeature = -1
    for i in range(numFeatures):
        &quot;&quot;&quot;
        创建唯一的分类标签列表
        &quot;&quot;&quot;
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)# 创建集合
        newEntropy = 0.0
        &quot;&quot;&quot;
        计算每种划分方式的信息熵
        第一列或第二列的划分方式 表3-1 将其他两列分类
        &quot;&quot;&quot;
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            &quot;&quot;&quot;
            将分完的两个类分别计算信息熵，乘以每个分类所出现的概率，相加后得到新的熵
            &quot;&quot;&quot;
            newEntropy += prob * calcuShannonEnt(subDataSet)
        &quot;&quot;&quot;
        判断信息增益是否大于0
        &quot;&quot;&quot;
        infoGain = baseEntropy - newEntropy
        if(infoGain &gt; bestInfoGain):
            &quot;&quot;&quot;
            计算最好的信息增益
            &quot;&quot;&quot;
            bestInfoGain = infoGain
            #  i为列数，对应表3-1
            bestFeature = i
    return bestFeature

&quot;&quot;&quot;
类似于投票表决的方法,挑选次数出现最多的类别
&quot;&quot;&quot;
def majorityCnt(classList):
    classCount={}
    for vote in classList:
        if vote not in classCount.key():
            classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]

&quot;&quot;&quot;
创建树的代码
&quot;&quot;&quot;
def createTree(dataSet,labels):
    # 把dataSet最后一列放到classList
    classList = [example[-1] for example in dataSet]
    #     print classList
    &quot;&quot;&quot;
    类别完全相同则停止继续划分
    &quot;&quot;&quot;
    #     对classList第一个元素进行统计，如果与总长度相同，表示只有一个分类
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    &quot;&quot;&quot;
    遍历完所有特征时返回出现次数最多的
    &quot;&quot;&quot;
    #     只剩最后一项的时候，按较多的
    #     print dataSet[0]
    #     计算dataSet第一个元素的长度
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    #     获取到最佳特征
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}}
    &quot;&quot;&quot;
    得到列表包含的所有属性值
    &quot;&quot;&quot;
    # 从标签里删除最佳特征标签
    del(labels[bestFeat])
    #将dataSet每个元素的第一列拿出
    featValues = [example[bestFeat] for example in dataSet]
    #使用set无重复的存入uniqueVals
    uniqueVals = set(featValues)
    for value in uniqueVals:
    #将标签复制一份，保证每次调用函数时不改变原始列表的内容，使用新变量代替原始列表
        subLabels = labels[:]
        #splitDataSet按当前分类方式进行分类，并将其他项作为新列表返回
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    return myTree

&quot;&quot;&quot;
使用决策树的分类函数
&quot;&quot;&quot;
def classify(inputTree,featLabels,testVec):
    firstStr = inputTree.keys()[0]
    secondDict = inputTree[firstStr]
    #将标签字符串转换成索引
    featIndex = featLabels.index(firstStr)
#     for key in secondDict.keys():
#         if testVec[featIndex] == key:
#             classLabel = classify(secondDict[key],featLabels,testVec)
#         else:
#             classLabel = secondDict[key]
    #下面是源码中的内容 可以运行
    key = testVec[featIndex]
    valueOfFeat = secondDict[key]
    if isinstance(valueOfFeat, dict): 
        # 非叶子节点继续递归判断
        classLabel = classify(valueOfFeat, featLabels, testVec)
    else: classLabel = valueOfFeat
    return classLabel
&quot;&quot;&quot;
决策树的存储，每次调用使用已经构造好的决策树，节省时间
&quot;&quot;&quot;
&quot;&quot;&quot;
使用pickle模块存储决策树
&quot;&quot;&quot;
def storeTree(inputTree,filename):
    import pickle
    fw = open(filename,&#39;w&#39;)
    #将对象保存到文件中
    pickle.dump(inputTree,fw)
    fw.close()

def grabTree(filename):
    import pickle
    fr = open(filename)
    #从文件中读取
    return pickle.load(fr)
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
测试决策树分类函数
&quot;&quot;&quot;
myDat,labels=createDataSet()
myTree=retrieveTree(0)
classify(myTree,labels,[1,1])
</code></pre>
<pre><code>&#39;yes&#39;
</code></pre><pre><code class="python">&quot;&quot;&quot;
测试存储效果
&quot;&quot;&quot;
myTree=retrieveTree(0)
storeTree(myTree,&#39;classfierStorage.txt&#39;)
grabTree(&#39;classfierStorage.txt&#39;)
</code></pre>
<pre><code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}
</code></pre><pre><code class="python">&quot;&quot;&quot;
生成决策树字典
&quot;&quot;&quot;
myData,labels = createDataSet()
myTree = createTree(myData,labels)
myTree
</code></pre>
<pre><code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}
</code></pre><pre><code class="python">&quot;&quot;&quot;
测试选择最好的数据集划分方式
&quot;&quot;&quot;
myData,labels = createDataSet()
chooseBestFeatureToSplit(myData)
</code></pre>
<pre><code>0
</code></pre><pre><code class="python">&quot;&quot;&quot;
测试使用创建的数据集计算信息熵
&quot;&quot;&quot;
myData,labels = createDataSet()
#增加分类熵变大
myData[0][-1]=&#39;maybe&#39;
calcuShannonEnt(myData)
</code></pre>
<pre><code>1.3709505944546687
</code></pre><pre><code class="python">&quot;&quot;&quot;
测试划分数据集
&quot;&quot;&quot;
myData,labels = createDataSet()
splitDataSet(myData,0,1)
</code></pre>
<pre><code>[[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]
</code></pre><pre><code class="python">&quot;&quot;&quot;
文件treePlotter
&quot;&quot;&quot;
&quot;&quot;&quot;
使用matplot文本注解回执树节点
&quot;&quot;&quot;
import matplotlib.pyplot as plt
&quot;&quot;&quot;
定义文本框和箭头格式
&quot;&quot;&quot;
decisionNode = dict(boxstyle=&quot;sawtooth&quot;, fc=&quot;0.8&quot;)
leafNode = dict(boxstyle=&quot;round4&quot;, fc=&quot;0.8&quot;)
arrow_args = dict(arrowstyle=&quot;&lt;-&quot;)
&quot;&quot;&quot;
绘制带箭头的注解
&quot;&quot;&quot;
def plotNode(nodeTxt, centerPt, parentPt, nodeType):
    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=&#39;axes fraction&#39;, \
                           xytext=centerPt, textcoords=&#39;axes fraction&#39;, \
                            va=&quot;center&quot;, ha=&quot;center&quot;, bbox=nodeType, arrowprops=arrow_args)
def createPlot1():
    fig = plt.figure(1, facecolor=&#39;white&#39;)
    fig.clf()
    createPlot.ax1 = plt.subplot(111, frameon=False)
    #decisionNode leafNode 是节点类型，不同节点类型样式不一样 包围文字的box不一样
    plotNode(&#39;a decision node&#39;, (0.5, 0.1), (0.1, 0.5), decisionNode)
    plotNode(&#39;a leaf node&#39;, (0.8, 0.1), (0.3, 0.8), leafNode)
    plt.show()

&quot;&quot;&quot;
画一个完整的树，我们需要知道有多少个叶节点（确定x）,树有多少层（确定y）
&quot;&quot;&quot;
#获取叶节点的数目
def getNumLeafs(myTree):
    numLeafs = 0
    #获取第一个键
    #树的第一层一定只有一个节点
    firstStr = myTree.keys()[0]
    #获取第一个键所对应的值
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        #判断节点数据（键的值）类型是否为字典
        if type(secondDict[key]).__name__==&#39;dict&#39;:
            numLeafs += getNumLeafs(secondDict[key])
        else:
            numLeafs += 1
    return numLeafs
#获取树的层数
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__==&#39;dict&#39;:
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:
            thisDepth = 1
        if thisDepth &gt; maxDepth : maxDepth = thisDepth
    return maxDepth

#预先存储下树的信息，避免每次测试都需要创建树，为了节约练习的时间
def retrieveTree(i):
    listOfTrees = [{&#39;no surfacing&#39;:
                    {0:&#39;no&#39;,
                     1:{&#39;flippers&#39;:
                        {0:&#39;no&#39;,1:&#39;yes&#39;}}}},
                  {&#39;no surfacing&#39;:
                   {0:&#39;no&#39;,1:{&#39;flippers&#39;:
                     {0:
                      {&#39;head&#39;:
                       {0:&#39;no&#39;,1:&#39;yes&#39;},1:&#39;no&#39;}}}}}]
    return listOfTrees[i]

&quot;&quot;&quot;
在父子节点间填充文本信息
&quot;&quot;&quot;
def plotMidText(cntrPt, parentPt, txtString):
    #计算文本信息的位置
    #① 0.5 1
    xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0]
    yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1]
    createPlot.ax1.text(xMid, yMid, txtString)

def plotTree(myTree, parentPt, nodeTxt):
    #计算宽与高
    numLeafs = getNumLeafs(myTree)
    depth = getTreeDepth(myTree)
    #获取根节点
    firstStr = myTree.keys()[0]
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)
    #① (0.5,1)  (0.5,1)  &quot;&quot;
    # 在父子节点之间添加文本信息
    plotMidText(cntrPt, parentPt, nodeTxt)
    #画节点: 节点内容 子节点坐标  父节点坐标  节点类型
    plotNode(firstStr, cntrPt, parentPt, decisionNode)
    # 第二层 
    secondDict = myTree[firstStr]
    # 修改y偏移量 1-1/2  有坐标范围0-1 这里从上到下绘制因此依次递减
    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD
    # 判断第二层节点下的节点是否为叶子节点
    for key in secondDict.keys():
        #不是叶子节点  递归执行
        if type(secondDict[key]).__name__==&#39;dict&#39;:
            plotTree(secondDict[key],cntrPt,str(key))
        #如果是叶子节点
        else:
            #计算x的偏移量 -1/8+1/4
            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW
            plotNode(secondDict[key], (plotTree.xOff,plotTree.yOff), cntrPt, leafNode)
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    # 1/2+1/2
    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD

def createPlot(inTree):
    #facecolor设置背景
    fig = plt.figure(1, facecolor=&#39;white&#39;)
    #清除
    fig.clf()
    #清空ticks，标线，这里应该就是坐标轴
    axprops = dict(xticks=[], yticks=[])
    #这个是没有ticks的
    #createPlot.ax1 = plt.subplot(111,frameon=False, **axprops)
    #这个是有ticks的
    createPlot.ax1 = plt.subplot(111,frameon=False)
    #将树的深度和叶子节点数保存为全局变量
    plotTree.totalW = float(getNumLeafs(inTree))
    print &quot;totalW: %f&quot; % plotTree.totalW
    plotTree.totalD = float(getTreeDepth(inTree))
    print &quot;totalD: %f&quot;% plotTree.totalD
    # -0.5/4
    plotTree.xOff = -0.5/plotTree.totalW;
    plotTree.yOff = 1.0
    #设置跟节点坐标
    plotTree(inTree, (0.5,1.0), &#39;&#39;)
    plt.show()
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
测试获取树的叶子节点数，树的层数
&quot;&quot;&quot;
myTree = retrieveTree(0)
getNumLeafs(myTree)
getTreeDepth(myTree)
</code></pre>
<pre><code>2
</code></pre><pre><code class="python">&quot;&quot;&quot;
创建树节点
&quot;&quot;&quot;
&quot;&quot;&quot;
前面的createPlot()为了做区分 改名为createPlot1
&quot;&quot;&quot;
createPlot1()
</code></pre>
<p><img src="/img/tree/output_9_0.png" alt="图像输出"></p>
<pre><code class="python">&quot;&quot;&quot;
获取树信息
&quot;&quot;&quot;
myTree=retrieveTree(0)
&quot;&quot;&quot;
创建树
&quot;&quot;&quot;
createPlot(myTree)
</code></pre>
<pre><code>totalW: 3.000000
totalD: 2.000000
</code></pre><p><img src="/img/tree/output_10_1.png" alt="图像输出"></p>
<pre><code class="python">&quot;&quot;&quot;
添加节点，测试输出效果
&quot;&quot;&quot;
myTree=retrieveTree(0)
myTree[&#39;no surfacing&#39;][3]=&#39;maybe&#39;
createPlot(myTree)
</code></pre>
<p><img src="/img/tree/output_11_0.png" alt="图像输出"></p>
<pre><code class="python">fr=open(&#39;F:\study\lenses.txt&#39;)
lences=[inst.strip().split(&#39;\t&#39;) for inst in fr.readlines()]
lensesLabels=[&#39;age&#39;,&#39;preascript&#39;,&#39;astigmatic&#39;,&#39;tearRate&#39;]
lensesTree = createTree(lences,lensesLabels)
lensesTree
</code></pre>
<pre><code>{&#39;tearRate&#39;: {&#39;normal&#39;: {&#39;astigmatic&#39;: {&#39;no&#39;: {&#39;age&#39;: {&#39;pre&#39;: &#39;soft&#39;,
      &#39;presbyopic&#39;: {&#39;preascript&#39;: {&#39;hyper&#39;: &#39;soft&#39;, &#39;myope&#39;: &#39;no lenses&#39;}},
      &#39;young&#39;: &#39;soft&#39;}},
    &#39;yes&#39;: {&#39;preascript&#39;: {&#39;hyper&#39;: {&#39;age&#39;: {&#39;pre&#39;: &#39;no lenses&#39;,
        &#39;presbyopic&#39;: &#39;no lenses&#39;,
        &#39;young&#39;: &#39;hard&#39;}},
      &#39;myope&#39;: &#39;hard&#39;}}}},
  &#39;reduced&#39;: &#39;no lenses&#39;}}
</code></pre><pre><code class="python">createPlot(lensesTree)
</code></pre>
<p><img src="/img/tree/output_13_0.png" alt="图像输出"></p>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/机器学习/">机器学习</a>
		  
			<a href="/tags/Python/">Python</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/09/08/机器学习实战（四）朴素贝叶斯（NaivaBayes）/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">机器学习实战（四）朴素贝叶斯（NaivaBayes）</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2017/09/08/机器学习实战（二）k-近邻算法/">
        <span class="next-text nav-default">机器学习实战（二）k-近邻算法</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">安 阳.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/frostfan/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    
  





  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
