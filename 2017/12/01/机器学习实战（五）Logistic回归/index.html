<!DOCTYPE html>
<html lang="">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="机器学习实战（五）Logistic回归"/>




  <meta name="keywords" content="机器学习,Python," />





  <link rel="alternate" href="/atom.xml" title="Russell">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1" />



<link rel="canonical" href="http://JohnneyAnn.github.io/2017/12/01/机器学习实战（五）Logistic回归/"/>


<meta name="description" content="Logistic回归是一种简单的分类算法，提到“回归”，很多人可能觉得与分类没什么关系，Logistic回归通过对数据分类边界的拟合来实现分类。而“回归”也就意味着最佳拟合。要进行最佳拟合，则需要寻找到最佳的拟合参数，一些最优化方法就可以用于最佳回归系数的确定。">
<meta name="keywords" content="机器学习,Python">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习实战（五）Logistic回归">
<meta property="og:url" content="http://JohnneyAnn.github.io/2017/12/01/机器学习实战（五）Logistic回归/index.html">
<meta property="og:site_name" content="Russell">
<meta property="og:description" content="Logistic回归是一种简单的分类算法，提到“回归”，很多人可能觉得与分类没什么关系，Logistic回归通过对数据分类边界的拟合来实现分类。而“回归”也就意味着最佳拟合。要进行最佳拟合，则需要寻找到最佳的拟合参数，一些最优化方法就可以用于最佳回归系数的确定。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://johnneyann.github.io/img/logistic/1.png">
<meta property="og:image" content="http://johnneyann.github.io/img/logistic/2.png">
<meta property="og:image" content="http://johnneyann.github.io/img/logistic/3.png">
<meta property="og:updated_time" content="2017-12-01T13:50:26.704Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习实战（五）Logistic回归">
<meta name="twitter:description" content="Logistic回归是一种简单的分类算法，提到“回归”，很多人可能觉得与分类没什么关系，Logistic回归通过对数据分类边界的拟合来实现分类。而“回归”也就意味着最佳拟合。要进行最佳拟合，则需要寻找到最佳的拟合参数，一些最优化方法就可以用于最佳回归系数的确定。">
<meta name="twitter:image" content="http://johnneyann.github.io/img/logistic/1.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1" />
<link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet'>


  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" />




<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: true
    },
  };
</script>




  



    <title> 机器学习实战（五）Logistic回归 - Russell </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">Russell</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          机器学习实战（五）Logistic回归
        
      </h1>

      <time class="post-time">
          12月 01 2017
      </time>
    </header>



    
            <div class="post-content">
            <p>Logistic回归是一种简单的分类算法，提到“回归”，很多人可能觉得与分类没什么关系，Logistic回归通过对数据分类边界的拟合来实现分类。而“回归”也就意味着最佳拟合。要进行最佳拟合，则需要寻找到最佳的拟合参数，一些最优化方法就可以用于最佳回归系数的确定。<br><a id="more"></a></p>
<h1 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h1><p><strong>对书中代码做的修改：</strong></p>
<ul>
<li>1.修改plotBestFit（wei）参数名称plotBestFit（weights）</li>
<li>2.移除plotBestFit（）中weights=wei.getA()</li>
<li>3.分析数据画决策边界调用plotBestFit（weights）时传入weights.getA()，其中weight是gredAscent(dataArr,labelMat)方法的返回值</li>
</ul>
<h2 id="LogistiC-回归梯度上升优化算法"><a href="#LogistiC-回归梯度上升优化算法" class="headerlink" title="LogistiC 回归梯度上升优化算法"></a>LogistiC 回归梯度上升优化算法</h2><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><pre><code class="python">from numpy import *
def loadDataSet():
    dataMat = []
    labelMat = []
    fr = open(&#39;F:\\study\\testSet.txt&#39;)
    for line in fr.readlines():
        #去除空格并拆分
        lineArr = line.strip().split()
        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return dataMat,labelMat
</code></pre>
<h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><pre><code class="python">&quot;&quot;&quot;
sigmoid函数
&quot;&quot;&quot;
def sigmoid(inX):
    return (1.0/(1+exp(-inX)))
</code></pre>
<h3 id="LogistiC-梯度上升算法"><a href="#LogistiC-梯度上升算法" class="headerlink" title="LogistiC 梯度上升算法"></a>LogistiC 梯度上升算法</h3><pre><code class="python">def gredAscent(dataMatIn, classLabels):
    #[[x,x,x],...]
    dataMatrix = mat(dataMatIn)
    #将列表转换为矩阵再倒置
    labelMat = mat(classLabels).transpose()
    #获取行列
    m,n = shape(dataMatrix)
    #向目标移动的步长
    alpha = 0.001
    #迭代次数
    maxCycles = 500
    #回归系数 n=3行1列的单位数组
    weights = ones((n,1))
    for k in range(maxCycles):
        #h&gt;0.5 在右边 dataMatrix*weights矩阵相乘100行一列
        #每次计算所有样本点
        h = sigmoid(dataMatrix*weights)
        error = (labelMat-h);
        #w = w + α▽w f(w)
        #dataMatrix.transpose() 3行100列  error 100行1列  相乘后3，1
        weights = weights + alpha * dataMatrix.transpose()*error
    return weights
</code></pre>
<ul>
<li>1.解析文本，文本中有100个样本点，前两列代表点的数值型特征X1,X2，最后一列为分类标签</li>
<li>2.设置回归系数初始值为1（为单位矩阵，每个样本点对应一个回归系数，设置回归系数行数等于样本点的列数，实现倒置），步长为0.001，训练次数500</li>
<li>3.计算z=wTx,带入sigmod获取结果，将结果与分类标签计算误差值</li>
<li>4.根据误差值方向调整回归系数（梯度迭代公式，梯度简化为数据样本与错误量相乘[见上篇博客公式推导]）</li>
</ul>
<pre><code class="python">&quot;&quot;&quot;
查看效果
&quot;&quot;&quot;
dataArr,labelMat = loadDataSet()
weights = gredAscent(dataArr,labelMat)
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
分析数据：画出决策边界
&quot;&quot;&quot;
import matplotlib.pyplot as plt
def plotBestFit(weights):
    dataMat,labelMat = loadDataSet()
    dataArr = array(dataMat)
    # n = 100
    n = shape(dataArr)[0]
    xcord1 = []; ycord1 = []
    xcord2 = []; ycord2 = []
    for i in range(n):
        if int(labelMat[i]) == 1:
            xcord1.append(dataArr[i, 1]); ycord1.append(dataArr[i,2])
        else:
            xcord2.append(dataArr[i, 1]); ycord2.append(dataArr[i,2])
    # 创建画布
    fig = plt.figure()
    ax = fig.add_subplot(111)
    # 画标签为1的点
    ax.scatter(xcord1, ycord1, s=30, c=&#39;red&#39;, marker=&#39;p&#39;)
    # 画标签为0的点
    ax.scatter(xcord2, ycord2, s=30, c=&#39;green&#39;)
    #创建决策边界线
    x = arange(-3.0, 3.0, 0.1)
    &quot;&quot;&quot;
    设置sigmod函数值为0,0是两个分类的分界处，可解出x，y的关系式
    &quot;&quot;&quot;
    # 0 = w0*x0 + w1*x + w2*y  =&gt;  y = -(w0 + w1*x)/w2
    y = (-weights[0]-weights[1]*x)/weights[2] #最佳拟合线
    ax.plot(x,y)
    plt.xlabel(&#39;X1&#39;);plt.ylabel(&#39;X2&#39;);
    plt.show()
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
查看效果
&quot;&quot;&quot;
from numpy import *
# 传入返回的回归系数(书中有误，因此修改了plotBestFit的传参，也为了后面随机梯度可共用
#另外需要定义weights将gredAscent的值赋给weights)
%matplotlib inline
# getA()将自身矩阵转化为ndarray类型的变量，等价于asarray(self)
plotBestFit(weights.getA())
</code></pre>
<p><img src="/img/logistic/1.png" alt="分析数据"></p>
<h2 id="随机梯度上升算法"><a href="#随机梯度上升算法" class="headerlink" title="随机梯度上升算法"></a>随机梯度上升算法</h2><pre><code class="python">&quot;&quot;&quot;
一次仅用一个样本点来更新回归系数
&quot;&quot;&quot;
def stocGradAscent0(dataMatrix, classLabels):
    m,n=shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)
    for i in range(m):
        #每次计算一个样本点
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha*error*dataMatrix[i]
    return weights
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
查看效果
&quot;&quot;&quot;
dataArr,labelMat = loadDataSet()
weights = stocGradAscent0(array(dataArr),labelMat)
plotBestFit(weights)
</code></pre>
<p><img src="/img/logistic/2.png" alt="分析数据"></p>
<h2 id="改进的随机梯度上升算法"><a href="#改进的随机梯度上升算法" class="headerlink" title="改进的随机梯度上升算法"></a>改进的随机梯度上升算法</h2><pre><code class="python">&quot;&quot;&quot;
改进的随机梯度上升算法
alpha在每次迭代的时候都会调整，随着迭代次数不断减小，但永远不会到0
每次从样本中随机取出一个样本更新回归系数，之后将改值从列表中删除，进行下次迭代
&quot;&quot;&quot;
def stocGradAscent1(dataMatrix, classLabels, numIter=150):
    m,n = shape(dataMatrix)
    weights = ones(n)
    for j in range(numIter):
        # 每个样本的编号[0,..,99]
        dataIndex = range(m)
        for i in range(m):
            &quot;&quot;&quot;
            步长alpha每次迭代时需要调整，每次减少1/(i+j)
            档j&lt;&lt;max(i),alpha就不是严格下降的。
            避免参数严格下降也常见于模拟退火算法（SAA)等其他优化算法中
            &quot;&quot;&quot;
            alpha = 4/(1.0+j+i)+0.01
            # numpy.random.uniform(low,high,size)在给定区间[0,100)内随机取样，注意是左闭右开
            # size输出样本数目，int或元组，缺省时输出一个值
            randIndex = int(random.uniform(0,len(dataIndex)))
            # 求和 即计算z = w0x0+w1*x1+w2*x2  随机选取样本计算h
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha * error * dataMatrix[randIndex]
            # 样本被使用后去除 在此次迭代内dataIndex长度每次减1
            del(dataIndex[randIndex])
    return weights
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
查看效果
&quot;&quot;&quot;
dataArr,labelMat = loadDataSet()
weights = stocGradAscent1(array(dataArr),labelMat)
plotBestFit(weights)
</code></pre>
<p><img src="/img/logistic/3.png" alt="分析数据"></p>
<h2 id="从疝气病症预测病马的死亡率"><a href="#从疝气病症预测病马的死亡率" class="headerlink" title="从疝气病症预测病马的死亡率"></a>从疝气病症预测病马的死亡率</h2><h3 id="处理数据中的缺失值"><a href="#处理数据中的缺失值" class="headerlink" title="处理数据中的缺失值"></a>处理数据中的缺失值</h3><p><strong>可选的做法</strong></p>
<blockquote>
<p>使用可用特征的均值来填补缺失值；<br>使用特殊值来填补缺失值，如-1;<br>忽略有缺失值的样本；<br>使用相似样本的均值添补缺失值；<br>使用另外的机器学习算法预测缺失值。</p>
</blockquote>
<p>numpy不允许包含缺失值，选择实数0来替换所有缺失值，恰好能适用于Logistic回归<br>我们需要的是一个在更新是不会影响系数的值，根据回归系数的更新公式<br>w = w + a <em> e </em> dataMatrix[randIndex],若dataMatrix的某特征对应值为0,则系数不做更新w=w，另外sigmoid(0)=0.5，对结果的预测不具有任何的倾向性</p>
<h3 id="Logistic回归分类函数"><a href="#Logistic回归分类函数" class="headerlink" title="Logistic回归分类函数"></a>Logistic回归分类函数</h3><pre><code class="python">def classifyVector(inX, weights):
    prob = sigmoid(sum(inX*weights))
    if prob &gt; 0.5: return 1.0
    else: return 0.0
def colicTest():
    frTrain = open(&#39;F:\study\horseColicTraining.txt&#39;)
    frTest = open(&#39;F:\study\horseColicTest.txt&#39;)
    trainingSet = []; trainingLabels = []
    for line in frTrain.readlines():
        currLine = line.strip().split(&#39;\t&#39;)
        lineArr = []
        for i in range(21):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[21]))
    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)
    errorCount = 0; numTestVec = 0.0
    for line in frTest.readlines():
        numTestVec += 1.0
        currLine = line.strip().split(&#39;\t&#39;)
        lineArr = []
        for i in range(21):
            lineArr.append(float(currLine[i]))
        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]):
            errorCount += 1
    errorRate = (float(errorCount)/numTestVec)
    print &quot;the error rate of this test is: %f&quot; % errorRate
    return errorRate
def multiTest():
    numTests = 10; errorSum=0.0
    for k in range(numTests):
        errorSum += colicTest()
    print &quot;after %d iterations the average error rate is: %f&quot; % (numTests, errorSum/float(numTests))
</code></pre>
<pre><code class="python">&quot;&quot;&quot;
运行时报D:\Anaconda2\lib\site-packages\ipykernel\__main__.py:5: 
RuntimeWarning: overflow encountered in exp警告
说明计算的数据结果溢出了，忽略也无妨，查过说可以调整sigmoid函数，
使用longfloat()来解决溢出，但没有解决，还在寻找解决办法。。
&quot;&quot;&quot;
multiTest()
</code></pre>
<pre><code>D:\Anaconda2\lib\site-packages\ipykernel\__main__.py:5: RuntimeWarning: overflow encountered in exp


the error rate of this test is: 0.298507
the error rate of this test is: 0.402985
the error rate of this test is: 0.268657
the error rate of this test is: 0.298507
the error rate of this test is: 0.283582
the error rate of this test is: 0.402985
the error rate of this test is: 0.388060
the error rate of this test is: 0.328358
the error rate of this test is: 0.388060
the error rate of this test is: 0.417910
after 10 iterations the average error rate is: 0.347761
</code></pre><h2 id="总结（来自书中）"><a href="#总结（来自书中）" class="headerlink" title="总结（来自书中）"></a>总结（来自书中）</h2><blockquote>
<p>Logistic回归的目的是寻找一个非线性函数sigmoid的最佳拟合参数，求解过程可以由最优化<br>算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机<br>梯度上升算法。<br>随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上<br>升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进<br>行批处理运算。<br>机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用<br>中的需求。现有一些解决方案，每种方案都各有优缺点。</p>
</blockquote>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/机器学习/">机器学习</a>
		  
			<a href="/tags/Python/">Python</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/12/17/后焦虑思考/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">后焦虑思考</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2017/11/27/Logistic回归部分公式推导/">
        <span class="next-text nav-default">Logistic回归部分公式推导</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">安 阳.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/frostfan/hexo-theme-polarbear">Polar Bear</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    
  





  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
